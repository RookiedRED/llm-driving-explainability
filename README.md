# ğŸš— LLM-Powered Explainable Autonomous Driving

An end-to-end system that bridges autonomous driving perception outputs
with Large Language Model (LLM) reasoning to generate structured,
grounded, and safety-aware driving explanations.

------------------------------------------------------------------------

## ğŸ¯ Motivation

Modern autonomous driving systems operate as complex
perception--planning--control pipelines.\
However, these systems often lack interpretable reasoning for their
decisions.

This project explores:

> How can we transform structured driving state representations into
> grounded, safety-aware natural language explanations using LLMs?

Instead of feeding raw images to an LLM, we design a **structured
driving state abstraction layer** that serves as a reliable interface
between perception outputs and reasoning.

------------------------------------------------------------------------

## ğŸ—ï¸ System Architecture

Dataset / Simulator (nuScenes / CARLA) â†“ Perception Layer - Object
detection - Tracking - Ego pose â†“ Driving State Abstraction - Distance
computation - Relative velocity - TTC (Time-to-collision) - Risk scoring
â†“ LLM Reasoning Layer - Structured prompt - JSON output schema - Safety
constraints â†“ Evaluation Layer - Groundedness check - Consistency
analysis - Latency benchmark

------------------------------------------------------------------------

## ğŸ“¦ Features

-   Structured driving state schema
-   Distance & risk feature extraction
-   TTC estimation
-   LLM-based explainability
-   Grounded JSON outputs
-   Hallucination detection
-   Latency benchmarking

------------------------------------------------------------------------

## ğŸ§  Design Principles

### 1. Simulator-Agnostic

The system does not depend on a specific simulator.\
It can ingest:

-   nuScenes dataset
-   CARLA
-   Synthetic scenarios

------------------------------------------------------------------------

### 2. Structured Interface (No Raw Vision to LLM)

Example state input:

``` json
{
  "ego_speed_kmh": 28,
  "objects": [
    {"type": "pedestrian", "distance_m": 8.2},
    {"type": "vehicle", "distance_m": 14.5}
  ],
  "traffic_light": "red",
  "ttc_s": 1.1,
  "risk_level": "high"
}
```

This enforces:

-   Reduced hallucination
-   Numerical grounding
-   Safer reasoning

------------------------------------------------------------------------

### 3. Evaluation-First Design

We evaluate:

-   Explanation groundedness
-   Action consistency
-   Safety rule alignment
-   Latency

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    llm-driving-explainability/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ ingest/
    â”‚   â”œâ”€â”€ perception/
    â”‚   â”œâ”€â”€ state/
    â”‚   â”œâ”€â”€ reasoning/
    â”‚   â”œâ”€â”€ eval/
    â”‚   â”œâ”€â”€ app/
    â”‚   â””â”€â”€ utils/
    â”œâ”€â”€ scripts/
    â”œâ”€â”€ data/
    â”œâ”€â”€ tests/
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install

``` bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Download Dataset

Download:

nuScenes v1.0-mini

Place under:

data/nuscenes/

Expected structure:

data/nuscenes/ samples/ sweeps/ maps/ v1.0-mini/

### 3ï¸âƒ£ Export Structured Driving States

``` bash
python scripts/export_states.py
```

------------------------------------------------------------------------

## ğŸ“Š Example Output

``` json
{
  "action": "brake",
  "explanation": [
    "Pedestrian detected ahead at 8.2m.",
    "Time-to-collision is 1.1 seconds.",
    "Collision risk classified as HIGH."
  ],
  "evidence": {
    "ttc_s": 1.1,
    "ego_speed_kmh": 28
  },
  "confidence": 0.87
}
```

------------------------------------------------------------------------

## ğŸ“ˆ Evaluation Metrics

-   Groundedness Score
-   Consistency Score
-   Rule Agreement Rate
-   Mean Latency (ms)

------------------------------------------------------------------------

## ğŸ›£ï¸ Roadmap

-   [ ] nuScenes integration
-   [ ] Risk feature engineering
-   [ ] LLM structured reasoning
-   [ ] Evaluation framework
-   [ ] Streamlit demo
-   [ ] CARLA integration
-   [ ] Edge case benchmark

------------------------------------------------------------------------

## âš ï¸ Disclaimer

This project is a research prototype and not intended for real-world
autonomous vehicle deployment.
